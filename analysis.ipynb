{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bfe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, y='Family') \n",
    "plt.title('Count of Each Ransomware Family')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b07474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, y='Threats')\n",
    "plt.title('Count of Each Threat')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83317432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, y='Prediction')\n",
    "plt.title('Count of Each Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca840a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Time'] >= 0]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ExpAddress'] != '1']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13b5d3",
   "metadata": {},
   "source": [
    "Splitting the data into a 70/30 split. 70% is left for the unlabeled pool, 30% is left for the unlabeled pool. In this unlabeled pool there is 20% reserved for a validation pool, and then 80% left for unlabeled training. This means the machine learning I am trying to achieve with this specific dataset is semi-supervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af47e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fix the column  typo\n",
    "df.rename(columns={'Protcol': 'Protocol'}, inplace=True)\n",
    "\n",
    "# Drop ID-like columns that won't be used in the model\n",
    "df_cleaned = df.drop(columns=['SeddAddress', 'ExpAddress', 'IPaddress'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_full = le.fit_transform(df_cleaned['Prediction'])\n",
    "\n",
    "\n",
    "# drop the original text 'Prediction' column\n",
    "features_df = df_cleaned.drop(columns=['Prediction'])\n",
    "\n",
    "# define columns to one-hot encode\n",
    "categorical_features = ['Protocol', 'Flag', 'Family', 'Threats']\n",
    "\n",
    "# create the encoded features DataFrame\n",
    "X_full = pd.get_dummies(features_df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "\n",
    "\n",
    "# separate 30% of the data to be  'labeled' pool\n",
    "X_labeled_pool, X_unlabeled, y_labeled_pool, y_unlabeled = train_test_split(\n",
    "    X_full, y_full, \n",
    "    test_size=0.70, # 70% of data becomes the 'unlabeled' set\n",
    "    random_state=42, \n",
    "    stratify=y_full\n",
    ")\n",
    "\n",
    "# split the labeled pool into training (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_labeled_pool, y_labeled_pool, \n",
    "    test_size=0.20, # 20% of the labeled pool becomes the validation set\n",
    "    random_state=42, \n",
    "    stratify=y_labeled_pool\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"--- Data Shapes ---\")\n",
    "print(f\"Labeled Training Set (X_train): {X_train.shape}\")\n",
    "print(f\"Validation Set (X_val):       {X_val.shape}\")\n",
    "print(f\"Unlabeled Set (X_unlabeled):  {X_unlabeled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# initialize the Random Forest Classifier\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# train model only on labeled data\n",
    "baseline_model.fit(X_train, y_train)\n",
    "# make predictions on the validation set\n",
    "predictions = baseline_model.predict(X_val)\n",
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "# detailed classification report\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e766d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get feature importances from the first model you trained\n",
    "importances = baseline_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the table\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd422ef",
   "metadata": {},
   "source": [
    "As we can see with the graph above, the importance scores show that there is no one singular strong performer, meaning there is little to no data leakage, and clusters (a normally important metric to check when it comes to data leakage) does not hold much of an importance significance meaning that the training model is mostly accurate and does not use any cheats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ee875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# get predicited probabilities for unlabeled data using baseline\n",
    "print(\"Making predictions on unlabeled data...\")\n",
    "unlabeled_probabilities = baseline_model.predict_proba(X_unlabeled)\n",
    "\n",
    "# find the model's confidence in its top prediction for each sample\n",
    "confidence_scores = np.max(unlabeled_probabilities, axis=1)\n",
    "\n",
    "# set a confidence threshold\n",
    "confidence_threshold = 0.98\n",
    "\n",
    "# filter for high-confidence predictions\n",
    "high_confidence_mask = confidence_scores > confidence_threshold\n",
    "X_pseudo_labeled = X_unlabeled[high_confidence_mask]\n",
    "pseudo_labels = baseline_model.predict(X_pseudo_labeled)\n",
    "\n",
    "print(f\"\\\\nFound {len(X_pseudo_labeled)} samples with >{confidence_threshold*100}% confidence.\")\n",
    "\n",
    "# combine the original training set with the new pseudo-labeled set\n",
    "X_combined_train = np.vstack([X_train, X_pseudo_labeled])\n",
    "y_combined_train = np.hstack([y_train, pseudo_labels])\n",
    "\n",
    "# train a new model on the larger, combined dataset\n",
    "print(\"\\\\nTraining new model on combined data...\")\n",
    "semi_supervised_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "semi_supervised_model.fit(X_combined_train, y_combined_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# evaluate the new model on the same validation set\n",
    "new_predictions = semi_supervised_model.predict(X_val)\n",
    "new_accuracy = accuracy_score(y_val, new_predictions)\n",
    "\n",
    "# compare results\n",
    "print(f\"\\\\nOriginal Baseline Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"New Semi-Supervised Model Accuracy: {new_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
